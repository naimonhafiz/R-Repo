---
title: "Multiple Linear Regression: A Complete Example"
author: "Naimon Hafiz"
runtime: shiny
output: html_document
---

```{r setup, include=FALSE}
# This chunk of code sets up the R session to perform the analysis
# Load packages, load data, load any other source scripts that may contain
# code or objects you will want to run to produce the report

# Load packages
library(asbio)
library(xtable)
library(shiny)
library(knitr)
library(DT)
require(scatterplot3d)
require(Hmisc)
require(rgl)
require(faraway)
library(car)
data(chredlin)
attach(chredlin)

# declare global chunk options
knitr::opts_chunk$set(echo = FALSE)

# determine output format dynamically
out_type <- knitr::opts_knit$get("rmarkdown.pandoc.to")

# define custom function for data label outputs
# The DT::datatable function is great for producing tables for HTML docs
# Otherwise, use the knitr::kable function to produce tables
# You should use the R help to learn about these two functions as they
# will need to be used to produce visually appealing tables for your
# report

display_output <- function(dataset, out_type, filter_opt = 'none') {
  
  if (out_type == "html") {
    out_table <- DT::datatable(dataset, filter = filter_opt)
  } else {
    out_table <- knitr::kable(dataset)
  } 
  
  out_table
}

```

## I. Introduction

*Description: Define the study, define \(Y_i\) and \(X_{ij}\), state the regression equation, model assumptions, sample size, and unknown parameters of interest. Articulate the study goals.* 

### A. Study Design. 

A study on insurance redlining is considered. To investigate charges by several Chicago community organizations that insurance companies were refusing to issue insurace to racial minorities, the U.S. Commission on Civil Rights gathered information on the number of FAIR plan policies written and renewed in Chicago (per 100 housing units, \(Y\)) by zip code for the months of December 1977 through May 1978.  FAIR plans were offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market.  Information on other variables that might also affect insurance writing were recorded. The variables are:  **race**, the racial composition in percentage of minority; **fire**, fires per 100 housing units; **theft**, thefts per 1000 population; **age**, percentage of housing units built before 1939; **income**, median family income in thousands of dollars; and **side**, North or South Side of Chicago


```{r describe}
# the display_output function was defined above, it's producing a table
# for each of the calls below
display_output(chredlin, out_type)
```

### B. Aims. 
The purpose of the study is to **investigate the relationship** between racial composition and insurance refusal in Chicago between December 1977 and May 1978 while controlling for other potential sources of variation.

### C. Statistical Model.
A multiple linear regression model is considered. Let

  \(Y_i = \) the number of FAIR plan policies written and renewed (per 100 housing units) for the \(i^{th}\) zip code

  \(X_{i1} =\) racial composition in percentage of minority for the \(i^{th}\) zip code, 

  \(X_{i2} = \) fires (per 100 housing units) for the \(i^{th}\) zip code,  
  
  \(X_{i3} =\) thefts (per 1000 population) for the \(i^{th}\) zip code,  
  
  \(X_{i4} = \) percentage of housing units built before 1939 for the \(i^{th}\) zip code,  
  
  \(X_{i5} =\) log median family income (in thousands of dollars) for the \(i^{th}\) zip code.  

The **initial model** is given by

\[Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \beta_4X_{i4} + \beta_5X_{i5} + \varepsilon_i\]

where \(\varepsilon_i \sim iidN(0,\sigma^2)\), \(i = 1, 2, . . . , 47\), and \(\beta_0, \beta_1, . . . , \beta_5,\) and \(\sigma^2\) are the unknown model parameters.

```{r echo=TRUE}
# Fit the initial model in order to refine it
m1 <- lm(involact~race + fire + theft + age + log(income))
```

## II. Preliminary Analyses.

*Description: Describe bivariate association(s) in the data using a scatterplot (matrix, if applicable), Pearson's correlation coefficient(s) \(r\), and a simple summary statement of observed relationships between variables. Mention any issues you may have identified with: transformations, outliers, etc. A summary of fit of candidate models should also be included here, along with fit statitics like \(R^2_{adj}, C^p, \) etc.*

### A. Bivariate Associations.
A scatterplot matrix indicates positive linear associations between all variables. 
```{r echo = TRUE}
pairs(involact~race+fire+theft+age+log(income),data=chredlin)
```

The Pearson correlation coefficients for all pairwise association are shown in Table 1.  Log(income) is highly associated with the covariate of interest (race).
```{r echo = TRUE}
cor(chredlin[,-7])
```

### B. Screening of Covariates and Verification of Assumptions
Based on automatic variable selection methods in combination with criterion-based statistics, log(income) was removed from the model. Partial residual plots, residual-versus-fitted plots, and measures of influence were investigated and no issues with high influence points, linearity, constant variance, independence, or normality were identified.

### C. Final Model
The **final model** is given by

\[Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \beta_4X_{i4} + \varepsilon_i\]

where \(\varepsilon_i \sim iidN(0,\sigma^2)\), \(i = 1, 2, . . . , 47\), and \(\beta_0, \beta_1, . . . , \beta_4,\) and \(\sigma^2\) are the unknown model parameters.

```{r echo = FALSE}
# Fit the final model in order to describe it
m2 <- lm(involact~race + fire + theft + age)
```

## III. Statistical Analysis.

*Description: Should follow the goals listed in Section I.  For each goal, write out the hypotheses being tested (if applicable) and the specific approach taken (e.g., \(F\) test or \(t\) test, 95% confidence interval, Bonferroni adjustment, value of \(\alpha\), etc.).*

The fitted model is displayed below. The rate of FAIR policies issued and renewed per 100 housing units increases, on average, `r summary(m2)$coef[2]` (95% CI `r confint(m2)[2,]`) for every 1% increase in minorities living in the zip code. Race explains `r partial.R2(lm(involact~fire+theft+age),m2)*100`% of the variation in the number of FAIR plan policies per 100 housing units issued and renewed, as compared to `r partial.R2(lm(involact~race+theft+age),m2)*100`% for fire rates, `r partial.R2(lm(involact~race+fire+age),m2)*100`% for rates of theft, and `r partial.R2(lm(involact~race+fire+theft),m2)*100`% for age.

```{r}
summary(m2)
anova(m2)
confint(m2)
```

## IV. Summary of Findings.

*Description: Restate major findings and provide proper interpretation within the context of the study design.*

There appears to be a positive relationship between FAIR plan policies issued and percentage minority of the population in zip codes. The limitations of this analysis include that it is done at the zip code level rather than at the family or person level.  Notice that the data is at the zip code level--an analysis of this data is unable to directly investigate whether minorities are denied insurance. This type of **ecological analysis** requires we assume that the chances a minority homeowner obtains a FAIR plan after adjusting for the effect of the other covariates is constant across all zip codes.  This assumption is not verifiable and may be violated, resulting in incorrect conclusions (called an **ecological fallacy**).

## V. Appendix

*Description: Include details of the model building process including: transformations, outliers, variable selection, assumption checking.*

### A. Diagnostics for Predictors
The purpose of this section is to examine the distribution of predictors, identify any unusually large or small values, and examine bivariate associations to identify multicollinearity.  Unusual values should be flagged as they may **influence** the fit of the model.  Bivariate associations between predictors could cause issues if the purpose of the model is estimation.

Strip plots for all predictors and the dependent variable (jittered) are shown next to boxplots of the same data. First, it should be acknowledged that a log transformation of **income** was taken.  Income, as expected, is positively skewed with most observations clustered together and a few observations at much higher income levels.  Income is considered in most analyses to be on a multiplicative scale, so because of that (and its skewed nature) a natural-log tranformation is appropriate.  

Other features of note:  there is a wide range of values for **race**, the covariate of interest in this problem. There is also skewness visible in the distributions of **theft** and **fire**, with observations clustered close to zero and a few data points with large values. We may need to apply transformations if the model diagnostics and assumption checks indicate it. 

```{r echo = TRUE}
for (i in 1:6){
  par(mfrow=c(1,2))
  stripchart(chredlin[,i], main = names(chredlin)[i],
                          vertical = T, method = "jitter")
  boxplot(chredlin[,i], main = names(chredlin)[i])
  par(mfrow=c(1,1))
}
```

### B. Screening of Predictors

1. **Added variable plots** for each of the covariates are shown. Added variable plots (also known as partial residual plots or adjusted variable plots) provide evidence of the importance of a covariate given the other covariates already in the model. They also display the nature of the relationship between the covariate and the outcome (i.e., linear, curvilinear, transformation necessary, etc.) and any problematic data points with respect to the predictor. The plots all indicate no need for transformations because linear relationships are apparent.  They also indicate each variable provides some added value to a model that already includes all other covariates because the slopes of the linear relationships are all appear to be non-zero.  

```{r echo = TRUE}
m1 <- lm(involact~race + fire + theft + age + log(income))
prplot(m1,1)
prplot(m1,2)
prplot(m1,3)
prplot(m1,4)
prplot(m1,5)
```

2. Since the purpose of the project is to examine the relationship between **race** and **involact** (racial minority percentage and rates of FAIR policies), the goal is estimating \(\beta_1\). **Multicollinearity** can create instability in estimation and so it should be avoided. A scatterplot matrix of the candidate covariates with race is shown. It appears that log(income) is highly associated with race (\(r = \)`r cor(log(income),race)`) and several other covariates.  

```{r echo=TRUE}
pairs(race~fire+theft+age+log(income))
cor(chredlin[,-c(5,7)])
```

3. **Automatic variable selection methods** can be a useful starting point in eliminating redundant variables. They should only be used as a guide to the screening and removal (or addition) of predictors. Here, **race** is forced to stay in the model and all other covariates are allowed to add or drop: 

```{r echo = TRUE}
library(leaps)
ma <- regsubsets(involact~race + fire + theft + age + log(income), force.in = 1, data = chredlin)
(sma <- summary(ma))
```
The summary output includes a matrix indicating which predictors are included in each of the 4 candidate models.  In the first model (first row of the matrix, indicated by a '2' for the number of predictors) with two predictors, only race and fire are included.  In the second model (row 2) with three (indicated by a '3') predictors, race, fire, and theft are included. The third and fourth models are in the last two rows of the matrix.

Several criteria for selecting the best model are produced, including \(R^2_{adj}\) (large values are better), Akaike Information Criterion (AIC) (smaller values are better), Bayes Information Criterion (BIC) (smaller values are better), and Mallow's \(C_p\) statistic (values of \(C_p\) close to \(p\) (number of beta coefficients).  Here, all statistics indicate that the best model is one in which log(income) is removed: \(R^2_{adj} = \) `r sma$adj[3]`, \(AIC = \) `r sma$bic[3]`, and \(C_p = \) `r sma$cp[3]`.  

```{r echo = TRUE}
sma$adj # Adjusted R2 big
plot(2:5,sma$adj, xlab = "Number of Parameters", ylab = expression(R^2[adj]))
sma$bic # BIC small
plot(2:5, sma$bic, xlab = "Number of Parameters", ylab = expression(BIC))
sma$cp # Cp = p
plot(2:5, sma$cp, xlab = "Number of Parameters", ylab = expression(C[p]))
```

```{r echo = TRUE}
# Fit the new model
m2 <- lm(involact~race + fire + theft + age)
```

4. Formal testing for **multicollinearity** can now be done using **variance inflation factors (VIF)**.  VIFs measure how much the variances of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related.  A maximum VIF in excess of 10 is a good rule of thumb for multicollinearity problems.  Based on the maximum VIF, `r max(vif(m2))`, there do not appear to be any issues that need remediating.  

```{r echo = TRUE}
vif(m2)
```

5. The preliminary model fit is shown below.

```{r echo = TRUE}
summary(m2)
```
If a quick check of assumptions and outliers shows no issues, this will be the final model.

### C. Residual Diagnostics

**Influence** refers to how an observation impacts the fit of a model.  Leave-one-out methods are good for identifying influential points--if the model fit with the point is very different than it is without the point, the point is said to be influential and the model is said to be sensitive to its inclusion.  To identify potentially influential points, Cook's distances (Cook's \(D\)) is extracted from the model and plotted using a half-normal plot to emphasize unusually large or small values:

```{r echo = TRUE}
m2i <- influence(m2) # Save influence stats
halfnorm(cooks.distance(m2))
chredlin[6,]
```
Observation 6 sticks out as potentially high influence.  A fit of the model without Observation 6 shows the model results do not change and the model can be considered robust to the data point.

```{r echo = TRUE}
summary(lm(involact~race+fire+theft+age, subset = -6))
```

The fitted-versus-residual plot looks like noise, with the exception of the diagonal streak in the plot near \(\hat{Y}=0\). This feature results from the large number of 0 response values in the data.  This plot supports normality and constant variance of the residuals.  A Q-Q plot supports normality, as well.  Linearity of the relationships were assessed and verified using the partial residual plots (above). 
```{r echo = TRUE}
plot(rstandard(m2)~predict(m2), xlab = expression(hat(Y)), ylab = "Studentized Deleted Residuals")
abline(h=0)
qqnorm(residuals(m2))
qqline(residuals(m2))
```

Error terms appear to be independent based on the sequence plot and formal correlation test.
```{r echo = TRUE}
plot(residuals(m2),type="l",ylab=expression(e[i]),main="Sequence Plot of Residuals")
points(residuals(m2),pch=16,col="darkgray")
abline(0,0,lty=2)
summary(lm(residuals(m1)[-1]~-1+residuals(m1)[-47]))
```